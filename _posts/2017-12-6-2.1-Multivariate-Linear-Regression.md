---
layout:     post
title:      2.1 Multivariate Linear Regression
subtitle:   
date:       2017-12-6
author:     SilverSpear1031
header-img: img/post-bg-mma-4.jpg
catalog: true
tags:
    - notes_ML/DL
---

<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>

# 2.1 Multivariate Linear Regression

## 1. Multiple Features

@(DM/ML)

Linear regression with multiple variables is also known as `"multivariate linear regression"`.

We now introduce notation for equations where we can have any number of input variables.
$$ \begin{align*}x_j^{(i)} &= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \newline x^{(i)}& = \text{the input (features) of the }i^{th}\text{ training example} \newline m &= \text{the number of training examples} \newline n &= \text{the number of features} \end{align*} $$

The multivariable form of the hypothesis function accommodating these multiple features is as follows:
$$ h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n $$

In order to develop intuition about this function, we can think about $$ \theta_0 $$ as the basic price of a house, $$ \theta_1 $$ as the price per square meter, $$ \theta_2 $$ as the price per floor, etc. $$ x_1 $$ will be the number of square meters in the house, $$ x_2 $$ the number of floors, etc.

Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:
$$ \begin{align*}h_\theta(x) =\begin{bmatrix}\theta_0 \hspace{2em} \theta_1 \hspace{2em} ... \hspace{2em} \theta_n\end{bmatrix}\begin{bmatrix}x_0 \newline x_1 \newline \vdots \newline x_n\end{bmatrix}= \theta^T x\end{align*} $$

This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.

`Remark: `Note that for convenience reasons in this course we assume $$ x_{0}^{(i)} =1 \text{ for } (i\in { 1,\dots, m } ) $$. This allows us to do matrix operations with theta and x. Hence making the two vectors '$$ \theta $$' and $$ x^{(i)} $$ match each other element-wise (that is, have the same number of elements: n+1).]

## 2. Gradient Descent for Multiple Variables

The gradient descent equation itself is generally the same form; we just have to repeat it for our 'n' features:
$$ \begin{align*} & \text{repeat until convergence:} \; \lbrace \newline \; & \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}\newline \; & \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \newline \; & \theta_2 := \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \newline & \cdots \newline \rbrace \end{align*} $$

In other words:
$$ \begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; & \text{for j := 0...n}\newline \rbrace\end{align*} $$

The following image compares gradient descent with one variable to gradient descent with multiple variables:
![1512528756482.png](http://45.77.14.203/Chevereto-Free/images/2017/12/25/1512528756482.png)

## 3. Gradient Descent in Practice I - `Feature Scaling`

We can `speed up gradient descent` by having each of our input values in roughly the same range. This is because `θ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.`

`It speeds up gradient descent by making it require fewer iterations to get to a good solution are reasons for using feature scaling, instead of speed up every single iteration!!`

The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:

−1 ≤ $$ x_{(i)} $$ ≤ 1

or

−0.5 ≤ $$ x_{(i)} $$ ≤ 0.5

These aren't exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few.

Two techniques to help with this are feature scaling and `mean normalization`. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:

$$ x_i := \dfrac{x_i - \mu_i}{s_i} $$

Where $$ μ_i $$ is the `average of all the values for feature (i)` and $$ s_i $$ is `the range of values (max - min)`, or $$ s_i $$ is the `standard deviation`.

Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation.

For example, if xi represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, 
$$ x_i := \dfrac{price-1000}{1900} $$

## 4. Gradient Descent in Practice II - `Learning Rate`
Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.

`Automatic convergence test. Declare convergence if J(θ) decreases by less than E in one iteration, where E is some small value such as $ 10^{−3} $. However in practice it's difficult to choose this threshold value.`

![1512589877466.png](http://45.77.14.203/Chevereto-Free/images/2017/12/25/1512589877466.png)

`It has been proven that if learning rate α is sufficiently small, then J(θ) will decrease on every iteration.`

![1512589888551.png](http://45.77.14.203/Chevereto-Free/images/2017/12/25/1512589888551.png)

To summarize:

If $$ \alpha $$ is too small: slow convergence.

If $$ \alpha $$ is too large: ￼may not decrease on every iteration and thus may not converge.

## 5. Features and Polynomial Regression

![1512591166697.png](http://45.77.14.203/Chevereto-Free/images/2017/12/25/1512591166697.png)

